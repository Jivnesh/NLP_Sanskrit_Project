{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict\n",
    "#run this code on a server, laptop will not be able to handle it. loop of 11,000*8,00,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"Extracted.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_sql(\"SELECT * FROM Sample\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metapath2 = pd.read_sql(\"SELECT * FROM metapath2\", conn)\n",
    "metapath3 = pd.read_sql(\"SELECT * FROM metapath3\", conn)\n",
    "metapath4 = pd.read_sql(\"SELECT * FROM metapath4\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('CNG_count.pickle', 'rb') as handle:\n",
    "    CNG_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('CNGG_count.pickle', 'rb') as handle:\n",
    "    CNGG_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Word_count.pickle', 'rb') as handle:\n",
    "    Word_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Lemma_count.pickle', 'rb') as handle:\n",
    "    Lemma_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innerdict = merge_two_dicts(CNG_count, CNGG_count)\n",
    "#we merge them for easier selection in the future\n",
    "#as denominators in metapath3,4 are either count of CNG or CNG_Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_dict(Type_1, Type_2):\n",
    "    with open(Type_1 + '|' + Type_2 + '.json', 'r') as fp:\n",
    "        d = json.load(fp)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = { 'LemmaLemma': from_dict('Lemma', 'Lemma'), 'LemmaWord' : from_dict('Lemma', 'Word'), 'LemmaCNG' : from_dict('Lemma', 'CNG'), 'LemmaCNG_Group' : from_dict('Lemma', 'CNG_Group'), 'WordLemma' : from_dict('Word', 'Lemma'), 'WordWord' : from_dict('Word', 'Word'), 'WordCNG' : from_dict('Word', 'CNG'), 'WordCNG_Group' : from_dict('Word', 'CNG_Group'), 'CNGLemma' : from_dict('CNG', 'Lemma'), 'CNGWord' : from_dict('CNG', 'Word'), 'CNGCNG' : from_dict('CNG', 'CNG'), 'CNGCNG_Group' : from_dict('CNG', 'CNG_Group'), 'CNG_GroupLemma' : from_dict('CNG_Group', 'Lemma'), 'CNG_GroupWord' : from_dict('CNG_Group', 'Word'), 'CNG_GroupCNG' : from_dict('CNG_Group', 'CNG'), 'CNG_GroupCNG_Group' : from_dict('CNG_Group', 'CNG_Group') } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = defaultdict(list)\n",
    "global features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use threading for the following operations, on a server\n",
    "\n",
    "def mp2(metapath2, sample):\n",
    "\n",
    "    for mp2i,mp2r in metapath2.iterrows():\n",
    "        #iterating through all metapaths of size 2\n",
    "        node1 = mp2r['node1']\n",
    "        node2 = mp2r['node2']\n",
    "        #since it is a metapath of size 2 we have only 2 nodes\n",
    "\n",
    "        vector = []\n",
    "\n",
    "        for i,r in sample.iterrows():\n",
    "            #iterating through 11,000 samples we selected\n",
    "            den1 = 0\n",
    "            #initializing denominator of bigram probability to 0\n",
    "            \n",
    "            #checking to see what type of External nodes are there in the metapath\n",
    "            #assigning den as per type of node\n",
    "            \n",
    "            if node1 == 'Lemma':\n",
    "                value1 = r['Entity_1'].split(\"_\")[0]\n",
    "                den1 = Lemma_count[value1]\n",
    "                part1 = 'Lemma'\n",
    "            elif node1 == 'CNG':\n",
    "                value1 = r['Entity_1'].split(\"_\")[1]\n",
    "                den1 = CNG_count[int(value1)]\n",
    "                part1 = 'CNG'\n",
    "            else:\n",
    "                value1 = r['Entity_1']\n",
    "                den1 = Word_count[value1]\n",
    "                part1 = 'Word'\n",
    "\n",
    "\n",
    "            if node2 == 'Lemma':\n",
    "                value2 = r['Entity_2'].split(\"_\")[0]\n",
    "                part2 = 'Lemma'\n",
    "            elif node2 == 'CNG':\n",
    "                value2 = r['Entity_2'].split(\"_\")[1]\n",
    "                part2 = 'CNG'\n",
    "            else:\n",
    "                value2 = r['Entity_2'] \n",
    "                part2 = 'Word'\n",
    "\n",
    "            if den1 == 0:\n",
    "                #this is okay as we will be considering metapaths with maximum unique values \n",
    "                prob12 = 0\n",
    "            else:\n",
    "                part12 = part1 + part2\n",
    "                num12 = graph[part12][str(value1 + '|' + value2)]\n",
    "                prob12 = num12/den1\n",
    "\n",
    "                prob = prob12\n",
    "\n",
    "            vector.append(prob)\n",
    "\n",
    "        features[node1+'|'+node2] = vector\n",
    "#Important: Word|Word column vector is our label vector ! for Mutual Information Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def mp3(metapath3, sample):\n",
    "    \n",
    "    for mp3i,mp3r in metapath3.iterrows():\n",
    "        #iterating through all metapaths of size 3\n",
    "        node1 = mp3r['node1']\n",
    "        node2 = mp3r['node2']\n",
    "        node3 = mp3r['node3']\n",
    "        #since it is a metapath of size 3 we have only 3 nodes\n",
    "        vector = []\n",
    "\n",
    "        for i,r in sample.iterrows():\n",
    "            den1 = 0\n",
    "            den2 = 0\n",
    "\n",
    "            if node1 == 'Lemma':\n",
    "                value1 = r['Entity_1'].split(\"_\")[0]\n",
    "                den = Lemma_count[value1]\n",
    "                part1 = 'Lemma'\n",
    "            elif node1 == 'CNG':\n",
    "                value1 = r['Entity_1'].split(\"_\")[1]\n",
    "                den = CNG_count[int(value1)]\n",
    "                part1 = 'CNG'\n",
    "            else:\n",
    "                value1 = r['Entity_1']\n",
    "                den = Word_count[value1]\n",
    "                part1 = 'Word'\n",
    "\n",
    "            value2 = node2\n",
    "            \n",
    "            try:\n",
    "                x = int(value2)\n",
    "                part2 = 'CNG'\n",
    "            except:\n",
    "                part2 = 'CNG_Group'\n",
    "                \n",
    "\n",
    "            if node3 == 'Lemma':\n",
    "                value3 = r['Entity_2'].split(\"_\")[0]\n",
    "                part3 = 'Lemma'\n",
    "            elif node3 == 'CNG':\n",
    "                value3 = r['Entity_2'].split(\"_\")[1]\n",
    "                part3 = 'CNG'\n",
    "            else:\n",
    "                value3 = r['Entity_2']\n",
    "                part3 = 'Word'\n",
    "\n",
    "            den2 = innerdict[node2]\n",
    "            #selecting from innerdict as it has counts of both CNG and CNG_Groups\n",
    "\n",
    "            if den1 == 0:\n",
    "                prob12 = 0\n",
    "            else:\n",
    "                part12 = part1 + part2\n",
    "                num12 = graph[part12][str(value1 + '|' + value2)]\n",
    "                prob12 = num12/den1               \n",
    "                \n",
    "            if den2 == 0:\n",
    "                prob23 = 0\n",
    "                \n",
    "            else:\n",
    "                part23 = part2 + part3\n",
    "                num23 = graph[part23][str(value2 + '|' + value3)]\n",
    "                prob23 = num23/den2\n",
    "\n",
    "            prob = prob12*prob23\n",
    "\n",
    "            vector.append(prob)\n",
    "\n",
    "        features[node1 + '|' + node2 + '|'+ node3]  = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mp4(metapath4, sample):\n",
    "    \n",
    "\n",
    "    for mp4i,mp4r in metapath4.iterrows():\n",
    "        #iterating through all metapaths of size 4\n",
    "        node1 = mp4r['node1']\n",
    "        node2 = mp4r['node2']\n",
    "        node3 = mp4r['node3']\n",
    "        node4 = mp4r['node4']\n",
    "        \n",
    "        #since it is a metapath of size 4 we have only 4 nodes\n",
    "        vector = []\n",
    "\n",
    "        for i,r in sample.iterrows():\n",
    "            den1 = 0\n",
    "            den2 = 0\n",
    "            den3 = 0\n",
    "\n",
    "            if node1 == 'Lemma':\n",
    "                value1 = r['Entity_1'].split(\"_\")[0]\n",
    "                den1 = Lemma_count[value1]\n",
    "                part1 = 'Lemma'\n",
    "            elif node1 == 'CNG':\n",
    "                value1 = r['Entity_1'].split(\"_\")[1]\n",
    "                den1 = CNG_count[int(value1)]\n",
    "                part1 = 'CNG'\n",
    "            else:\n",
    "                value1 = r['Entity_1']\n",
    "                den1 = Word_count[value1]\n",
    "                part1 = 'Word'\n",
    "\n",
    "            value2 = node2\n",
    "            try:\n",
    "                x = int(value2)\n",
    "                part2 = 'CNG'\n",
    "            except:\n",
    "                part2 = 'CNG_Group'\n",
    "\n",
    "            value3 = node3\n",
    "            try:\n",
    "                x = int(value3)\n",
    "                part3 = 'CNG'\n",
    "            except:\n",
    "                part3 = 'CNG_Group'\n",
    "                \n",
    "            \n",
    "            if node4 == 'Lemma':\n",
    "                value4 = r['Entity_2'].split(\"_\")[0]\n",
    "                part4 = 'Lemma'\n",
    "            elif node4 == 'CNG':\n",
    "                value4 = r['Entity_2'].split(\"_\")[1]\n",
    "                part4 = 'CNG'\n",
    "            else:\n",
    "                value4 = r['Entity_2']\n",
    "                part4 = 'Word'\n",
    "\n",
    "            den2 = innerdict[node2]\n",
    "            den3 = innerdict[node3]\n",
    "\n",
    "            if den1 == 0:\n",
    "                prob12 = 0\n",
    "            else:\n",
    "                part12 = part1 + part2\n",
    "                num12 = graph[part12][str(value1 + '|' + value2)]\n",
    "                prob12 = num12/den1 \n",
    "\n",
    "            if den2 == 0:\n",
    "                prob23 = 0\n",
    "            else:\n",
    "                part23 = part2 + part3\n",
    "                num23 = graph[part23][str(value2 + '|' + value3)]\n",
    "                prob23 = num23/den2\n",
    "\n",
    "            if den3 == 0:\n",
    "                prob34 = 0\n",
    "            else:\n",
    "                part34 = part3 + part4\n",
    "                num34 = graph[part34][str(value3 + '|' + value4)]\n",
    "                prob34 = num34/den3\n",
    "\n",
    "\n",
    "            prob = prob12*prob23*prob34\n",
    "\n",
    "            vector.append(prob)\n",
    "\n",
    "        features[node1 + '|' + node2 + '|'+ node3 + '|' +node4]  = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Threading begins here\n",
    "#Calculate Length of Metapahts so that we can slpit them for threading \n",
    "lenmp2 = len(metapath2.node1)\n",
    "lenmp3 = len(metapath3.node1)\n",
    "lenmp4 = len(metapath4.node1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Running a total of 20 threads , mp2 : 1 , mp3: 1, mp4 : 18 threads respectivly \n",
    "\n",
    "step4 = int(lenmp4/18)\n",
    "\n",
    "ind4=[]\n",
    "z =0\n",
    "for i in range(17):\n",
    "    ind4.append([z, z+step4])\n",
    "    z = z+step4\n",
    "ind4.append([z, lenmp4-1])\n",
    "\n",
    "    \n",
    "t1 = threading.Thread(target=mp2, args=(metapath2, sample))\n",
    "\n",
    "t2 = threading.Thread(target=mp3, args=(metapath3, sample))\n",
    "\n",
    "t3 = threading.Thread(target=mp3, args=(metapath4[ind4[1][0]:ind4[1][1], :], sample))\n",
    "t4 = threading.Thread(target=mp3, args=(metapath4[ind4[2][0]:ind4[2][1], :], sample))\n",
    "t5 = threading.Thread(target=mp3, args=(metapath4[ind4[3][0]:ind4[3][1], :], sample))\n",
    "t6 = threading.Thread(target=mp3, args=(metapath4[ind4[4][0]:ind4[4][1], :], sample))\n",
    "t7 = threading.Thread(target=mp4, args=(metapath4[ind4[5][0]:ind4[5][1], :], sample))\n",
    "t8 = threading.Thread(target=mp4, args=(metapath4[ind4[6][0]:ind4[6][1], :], sample))\n",
    "t9 = threading.Thread(target=mp4, args=(metapath4[ind4[7][0]:ind4[7][1], :], sample))\n",
    "t10 = threading.Thread(target=mp4, args=(metapath4[ind4[8][0]:ind4[8][1], :], sample))\n",
    "t11 = threading.Thread(target=mp4, args=(metapath4[ind4[9][0]:ind4[9][1], :], sample))\n",
    "t12 = threading.Thread(target=mp4, args=(metapath4[ind4[10][0]:ind4[10][1], :], sample))\n",
    "t13 = threading.Thread(target=mp4, args=(metapath4[ind4[11][0]:ind4[11][1], :], sample))\n",
    "t14 = threading.Thread(target=mp4, args=(metapath4[ind4[12][0]:ind4[12][1], :], sample))\n",
    "t15 = threading.Thread(target=mp4, args=(metapath4[ind4[13][0]:ind4[13][1], :], sample))\n",
    "t16 = threading.Thread(target=mp4, args=(metapath4[ind4[14][0]:ind4[14][1], :], sample))\n",
    "t17 = threading.Thread(target=mp4, args=(metapath4[ind4[15][0]:ind4[15][1], :], sample))\n",
    "t18 = threading.Thread(target=mp4, args=(metapath4[ind4[16][0]:ind4[16][1], :], sample))\n",
    "t19 = threading.Thread(target=mp4, args=(metapath4[ind4[17][0]:ind4[17][1], :], sample))\n",
    "t20 = threading.Thread(target=mp4, args=(metapath4[ind4[18][0]:ind4[18][1], :], sample))\n",
    "\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t5.start()\n",
    "t6.start()\n",
    "t7.start()\n",
    "t8.start()\n",
    "t9.start()\n",
    "t10.start()\n",
    "t11.start()\n",
    "t12.start()\n",
    "t13.start()\n",
    "t14.start()\n",
    "t15.start()\n",
    "t16.start()\n",
    "t17.start()\n",
    "t18.start()\n",
    "t19.start()\n",
    "t20.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "t3.join()\n",
    "t4.join()\n",
    "t5.join()\n",
    "t6.join()\n",
    "t7.join()\n",
    "t8.join()\n",
    "t9.join()\n",
    "t10.join()\n",
    "t11.join()\n",
    "t12.join()\n",
    "t13.join()\n",
    "t14.join()\n",
    "t15.join()\n",
    "t16.join()\n",
    "t17.join()\n",
    "t18.join()\n",
    "t19.join()\n",
    "t20.join()\n",
    "\n",
    "#tip : use ctrl + left mouse click to have cursir select multiple lines and change at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'Features.json', 'w') as fp:\n",
    "        json.dump(features, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn1 = sqlite3.connect(\"Features.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(features, index = [i for i in range(len(sample.index))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe.to_sql(\"FeaturesRaw\", conn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()\n",
    "conn1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
