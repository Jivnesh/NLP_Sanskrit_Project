{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this part we have done the exact same thing as UniqueValues except this time insted of using 20 Lakh metapaths we use \n",
    "top 2 Lakh and recalcute all the vaules and person co-efficent with respect to Word|Word metapath as our label.\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from multiprocessing.managers import BaseManager, DictProxy\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"Extracted.db\")\n",
    "conn1=sqlite3.connect(\"Features.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#During Multiprocesing as Python is built on C, defaultdict is not defined so we are formally defining it\n",
    "class MyManager(BaseManager):\n",
    "    pass\n",
    "\n",
    "MyManager.register('defaultdict', defaultdict, DictProxy)\n",
    "#registering defualtdict into our manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mgr = MyManager()\n",
    "mgr.start()\n",
    "features = mgr.defaultdict(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_sql(\"SELECT * FROM SampleZero\", conn)\n",
    "#we dont have any metapaths of length 2 after initial stages of filtering\n",
    "metapath3 = pd.read_sql(\"SELECT * FROM Meta3_filteredPearson\", conn1)\n",
    "metapath4 = pd.read_sql(\"SELECT * FROM Meta4_filteredPearson\", conn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('CNG_count.pickle', 'rb') as handle:\n",
    "    CNG_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('CNGG_count.pickle', 'rb') as handle:\n",
    "    CNGG_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Word_count.pickle', 'rb') as handle:\n",
    "    Word_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Lemma_count.pickle', 'rb') as handle:\n",
    "    Lemma_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innerdict = merge_two_dicts(CNG_count, CNGG_count)\n",
    "#we merge them for easier selection in the future\n",
    "#as denominators in metapath3,4 are either count of CNG or CNG_Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_dict(Type_1, Type_2):\n",
    "    with open(Type_1 + '|' + Type_2 + '.json', 'r') as fp:\n",
    "        d = json.load(fp)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = { 'LemmaLemma': from_dict('Lemma', 'Lemma'), 'LemmaWord' : from_dict('Lemma', 'Word'), 'LemmaCNG' : from_dict('Lemma', 'CNG'), 'LemmaCNG_Group' : from_dict('Lemma', 'CNG_Group'), 'WordLemma' : from_dict('Word', 'Lemma'), 'WordWord' : from_dict('Word', 'Word'), 'WordCNG' : from_dict('Word', 'CNG'), 'WordCNG_Group' : from_dict('Word', 'CNG_Group'), 'CNGLemma' : from_dict('CNG', 'Lemma'), 'CNGWord' : from_dict('CNG', 'Word'), 'CNGCNG' : from_dict('CNG', 'CNG'), 'CNGCNG_Group' : from_dict('CNG', 'CNG_Group'), 'CNG_GroupLemma' : from_dict('CNG_Group', 'Lemma'), 'CNG_GroupWord' : from_dict('CNG_Group', 'Word'), 'CNG_GroupCNG' : from_dict('CNG_Group', 'CNG'), 'CNG_GroupCNG_Group' : from_dict('CNG_Group', 'CNG_Group') } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CNG_Distinct = len(CNG_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining the label, which is Word|Word\n",
    "label = []\n",
    "for i,r in sample.iterrows():\n",
    "    value1 = r['Entity_1']\n",
    "    value2 = r['Entity_2'] \n",
    "    try:\n",
    "        num = graph['WordWord'][value1 + '|' + value2]\n",
    "    except:\n",
    "        num = 0\n",
    "    den = Word_count[value1]\n",
    "    prob = (float(num) + 1) /(den + CNG_Distinct)\n",
    "    label.append(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mp3(features, metapath3, sample):\n",
    "    for mp3i,mp3r in metapath3.iterrows():\n",
    "        #iterating through all metapaths of size 3\n",
    "        node1 = mp3r['node1']\n",
    "        node2 = mp3r['node2']\n",
    "        node3 = mp3r['node3']\n",
    "        print(node1, node2, node3)        \n",
    "        #since it is a metapath of size 3 we have only 3 nodes\n",
    "        vector = []\n",
    "        for i,r in sample.iterrows():\n",
    "            den1 = 0\n",
    "            den2 = 0\n",
    "            if node1 == 'Lemma':\n",
    "                value1 = r['Entity_1'].split(\"_\")[0]\n",
    "                den1 = Lemma_count[value1]\n",
    "                part1 = 'Lemma'\n",
    "            elif node1 == 'CNG':\n",
    "                value1 = r['Entity_1'].split(\"_\")[1]\n",
    "                den1 = CNG_count[value1]\n",
    "                part1 = 'CNG'\n",
    "            else:\n",
    "                value1 = r['Entity_1']\n",
    "                den = Word_count[value1]\n",
    "                part1 = 'Word'\n",
    "            value2 = node2            \n",
    "            try:\n",
    "                x = int(value2)\n",
    "                part2 = 'CNG'\n",
    "            except:\n",
    "                part2 = 'CNG_Group' \n",
    "            den2 = innerdict[node2]               \n",
    "            if node3 == 'Lemma':\n",
    "                value3 = r['Entity_2'].split(\"_\")[0]\n",
    "                part3 = 'Lemma'\n",
    "            elif node3 == 'CNG':\n",
    "                value3 = r['Entity_2'].split(\"_\")[1]\n",
    "                part3 = 'CNG'\n",
    "            else:\n",
    "                value3 = r['Entity_2']\n",
    "                part3 = 'Word'\n",
    "            #selecting from innerdict as it has counts of both CNG and CNG_Groups\n",
    "            part12 = part1 + part2\n",
    "            try:                    \n",
    "                num12 = graph[part12][value1 + '|' + value2]\n",
    "            except:\n",
    "                num12 = 0\n",
    "            prob12 = (float(num12) + 1)/(den1 + CNG_Distinct)                              \n",
    "            part23 = part2 + part3\n",
    "            try:\n",
    "                num23 = graph[part23][value2 + '|' + value3]\n",
    "            except:\n",
    "                num23 = 0\n",
    "            prob23 = (float(num23) + 1)/(den2 + CNG_Distinct)\n",
    "            prob = prob12*prob23\n",
    "            vector.append(prob)\n",
    "        features[node1 + '|' + node2 + '|'+ node3]  = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mp4(features ,metapath4, sample):\n",
    "    for mp4i,mp4r in metapath4.iterrows():\n",
    "        #iterating through all metapaths of size 4\n",
    "        node1 = mp4r['node1']\n",
    "        node2 = mp4r['node2']\n",
    "        node3 = mp4r['node3']\n",
    "        node4 = mp4r['node4']\n",
    "        #since it is a metapath of size 4 we have only 4 nodes\n",
    "        vector = []\n",
    "        for i,r in sample.iterrows():\n",
    "            den1 = 0\n",
    "            den2 = 0\n",
    "            den3 = 0\n",
    "            if node1 == 'Lemma':\n",
    "                value1 = r['Entity_1'].split(\"_\")[0]\n",
    "                den1 = Lemma_count[value1]\n",
    "                part1 = 'Lemma'\n",
    "            elif node1 == 'CNG':\n",
    "                value1 = r['Entity_1'].split(\"_\")[1]\n",
    "                den1 = CNG_count[value1]\n",
    "                part1 = 'CNG'\n",
    "            else:\n",
    "                value1 = r['Entity_1']\n",
    "                den1 = Word_count[value1]\n",
    "                part1 = 'Word'\n",
    "            value2 = node2\n",
    "            try:\n",
    "                x = int(value2)\n",
    "                part2 = 'CNG'\n",
    "            except:\n",
    "                part2 = 'CNG_Group'\n",
    "            value3 = node3\n",
    "            try:\n",
    "                x = int(value3)\n",
    "                part3 = 'CNG'\n",
    "            except:\n",
    "                part3 = 'CNG_Group'       \n",
    "            if node4 == 'Lemma':\n",
    "                value4 = r['Entity_2'].split(\"_\")[0]\n",
    "                part4 = 'Lemma'\n",
    "            elif node4 == 'CNG':\n",
    "                value4 = r['Entity_2'].split(\"_\")[1]\n",
    "                part4 = 'CNG'\n",
    "            else:\n",
    "                value4 = r['Entity_2']\n",
    "                part4 = 'Word'\n",
    "            den2 = innerdict[node2]\n",
    "            den3 = innerdict[node3]\n",
    "            part12 = part1 + part2\n",
    "            try:\n",
    "                num12 = graph[part12][value1 + '|' + value2]\n",
    "            except:\n",
    "                num12 = 0\n",
    "            prob12 = (float(num12) + 1)/(den1 + CNG_Distinct)\n",
    "            part23 = part2 + part3\n",
    "            try:\n",
    "                num23 = graph[part23][value2 + '|' + value3]\n",
    "            except:\n",
    "                num23 = 0\n",
    "            prob23 = (float(num23) + 1)/(den2 + CNG_Distinct)\n",
    "            part34 = part3 + part4\n",
    "            try:\n",
    "                num34 = graph[part34][value3 + '|' + value4]\n",
    "            except:\n",
    "                num34 = 0\n",
    "            prob34 = (float(num34) + 1)/(den3 + CNG_Distinct)\n",
    "            prob = prob12*prob23*prob34\n",
    "            vector.append(prob)\n",
    "        features[node1 + '|' + node2 + '|'+ node3 + '|' +node4]  = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Threading begins here\n",
    "#Calculate Length of Metapahts so that we can slpit them for threading \n",
    "lenmp3 = len(metapath3.node1)\n",
    "lenmp4 = len(metapath4.node1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Running a total of 40 processes on 6 servers, mp2 : 1 Process , mp3: 1 Process, mp4 : 40 Processes on 6 servers\n",
    "\n",
    "serverno = 1\n",
    "server = 0\n",
    "#we change the value of server accroding to which server we are using \n",
    "step4 = int(lenmp4/(40*serverno))\n",
    "\n",
    "ind4=[]\n",
    "\n",
    "z = server*40*step4\n",
    "\n",
    "#make sure to change range from 40 to 39, and include the commented line of code in last server.\n",
    "\n",
    "for i in range(39):\n",
    "    ind4.append([z, z+step4])\n",
    "    z = z+step4\n",
    "ind4.append([z,lenmp4-1])\n",
    "\n",
    "print ind4\n",
    "\n",
    "mgr = MyManager()\n",
    "mgr.start()\n",
    "#initalizing the manager and start it.\n",
    "\n",
    "features = mgr.defaultdict(list)\n",
    "\n",
    "#run mp3 only on one server\n",
    "\n",
    "mp3(features ,metapath3, sample)\n",
    "\n",
    "t1 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[0][0]:ind4[0][1], :], sample))\n",
    "t2 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[1][0]:ind4[1][1], :], sample))\n",
    "t3 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[2][0]:ind4[2][1], :], sample))\n",
    "t4 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[3][0]:ind4[3][1], :], sample))\n",
    "t5 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[4][0]:ind4[4][1], :], sample))\n",
    "t6 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[5][0]:ind4[5][1], :], sample))\n",
    "t7 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[6][0]:ind4[6][1], :], sample))\n",
    "t8 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[7][0]:ind4[7][1], :], sample))\n",
    "t9 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[8][0]:ind4[8][1], :], sample))\n",
    "t10 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[9][0]:ind4[9][1], :], sample))\n",
    "t11 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[10][0]:ind4[10][1], :], sample))\n",
    "t12 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[11][0]:ind4[11][1], :], sample))\n",
    "t13 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[12][0]:ind4[12][1], :], sample))\n",
    "t14 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[13][0]:ind4[13][1], :], sample))\n",
    "t15 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[14][0]:ind4[14][1], :], sample))\n",
    "t16 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[15][0]:ind4[15][1], :], sample))\n",
    "t17 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[16][0]:ind4[16][1], :], sample))\n",
    "t18 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[17][0]:ind4[17][1], :], sample))\n",
    "t19 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[18][0]:ind4[18][1], :], sample))\n",
    "t20 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[19][0]:ind4[19][1], :], sample))\n",
    "t21 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[20][0]:ind4[20][1], :], sample))\n",
    "t22 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[21][0]:ind4[21][1], :], sample))\n",
    "t23 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[22][0]:ind4[22][1], :], sample))\n",
    "t24 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[23][0]:ind4[23][1], :], sample))\n",
    "t25 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[24][0]:ind4[24][1], :], sample))\n",
    "t26 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[25][0]:ind4[25][1], :], sample))\n",
    "t27 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[26][0]:ind4[26][1], :], sample))\n",
    "t28 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[27][0]:ind4[27][1], :], sample))\n",
    "t29 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[28][0]:ind4[28][1], :], sample))\n",
    "t30 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[29][0]:ind4[29][1], :], sample))\n",
    "t31 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[30][0]:ind4[30][1], :], sample))\n",
    "t32 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[31][0]:ind4[31][1], :], sample))\n",
    "t33 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[32][0]:ind4[32][1], :], sample))\n",
    "t34 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[33][0]:ind4[33][1], :], sample))\n",
    "t35 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[34][0]:ind4[34][1], :], sample))\n",
    "t36 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[35][0]:ind4[35][1], :], sample))\n",
    "t37 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[36][0]:ind4[36][1], :], sample))\n",
    "t38 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[37][0]:ind4[37][1], :], sample))\n",
    "t39 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[38][0]:ind4[38][1], :], sample))\n",
    "t40 = multiprocessing.Process(target=mp4, args=(features,metapath4.iloc[ind4[39][0]:ind4[39][1], :], sample))\n",
    "\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t5.start()\n",
    "t6.start()\n",
    "t7.start()\n",
    "t8.start()\n",
    "t9.start()\n",
    "t10.start()\n",
    "t11.start()\n",
    "t12.start()\n",
    "t13.start()\n",
    "t14.start()\n",
    "t15.start()\n",
    "t16.start()\n",
    "t17.start()\n",
    "t18.start()\n",
    "t19.start()\n",
    "t20.start()\n",
    "t21.start()\n",
    "t22.start()\n",
    "t23.start()\n",
    "t24.start()\n",
    "t25.start()\n",
    "t26.start()\n",
    "t27.start()\n",
    "t28.start()\n",
    "t29.start()\n",
    "t30.start()\n",
    "t31.start()\n",
    "t32.start()\n",
    "t33.start()\n",
    "t34.start()\n",
    "t35.start()\n",
    "t36.start()\n",
    "t37.start()\n",
    "t38.start()\n",
    "t39.start()\n",
    "t40.start()\n",
    "\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "t3.join()\n",
    "t4.join()\n",
    "t5.join()\n",
    "t6.join()\n",
    "t7.join()\n",
    "t8.join()\n",
    "t9.join()\n",
    "t10.join()\n",
    "t11.join()\n",
    "t12.join()\n",
    "t13.join()\n",
    "t14.join()\n",
    "t15.join()\n",
    "t16.join()\n",
    "t17.join()\n",
    "t18.join()\n",
    "t19.join()\n",
    "t20.join()\n",
    "t21.join()\n",
    "t22.join()\n",
    "t23.join()\n",
    "t24.join()\n",
    "t25.join()\n",
    "t26.join()\n",
    "t27.join()\n",
    "t28.join()\n",
    "t29.join()\n",
    "t30.join()\n",
    "t31.join()\n",
    "t32.join()\n",
    "t33.join()\n",
    "t34.join()\n",
    "t35.join()\n",
    "t36.join()\n",
    "t37.join()\n",
    "t38.join()\n",
    "t39.join()\n",
    "t40.join()\n",
    "\n",
    "#next time use pooling insted of writing so many processes\n",
    "#tip : use ctrl + left mouse click to have cursor select multiple lines and change at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'PearsonFeatures.json', 'w') as fp:\n",
    "    json.dump(features._getvalue(), fp)\n",
    "mgr.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
